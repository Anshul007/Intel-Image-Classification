{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import glob as glob\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "#from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "## Checking for device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([\n",
    "    # resize all the images to 150*150\n",
    "    transforms.Resize((150,150)),\n",
    "    \n",
    "    # This has a 50% probability of the image is either original or horizontali flip\n",
    "    transforms.RandomHorizontalFlip(), # so it will end up increasing the images by the factor of 2\n",
    "    \n",
    "    # change the color pixel in the range of 0-255 to 0-1, also change the data type from numpy to tensor\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # transforms the range from 0-1 to -1-1, and in 2*3 matrix row= RGB Channel and Column= mean & Standard Deviation\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], # new pixels = (X - mean)/ Std\n",
    "                        [0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path for Testing and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/home/Users/anshulchaurasia/Downloads/HackerEarth/Intel Image Classification/seg_train\"\n",
    "test_path = \"/home/Users/anshulchaurasia/Downloads/HackerEarth/Intel Image Classification/seg_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Note:</h3> When we have the lage number of images the we have to load them in <b>batches</b>. Because when we will trying loading all the images at once we will endup with <b>O</b>ut <b>O</b>f <b>M</b>emory error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(torchvision.datasets.ImageFolder(train_path, transform= transformer),\n",
    "                         batch_size = 256, # higher the batch size can lead the memory error\n",
    "                         shuffle = True # will remove the model bias in case the order of images will be same\n",
    "                         )\n",
    "\n",
    "test_loader = DataLoader(torchvision.datasets.ImageFolder(test_path, transform= transformer),\n",
    "                         batch_size = 256, # higher the batch size can lead the memory error\n",
    "                         shuffle = True # will remove the model bias in case the order of images will be same\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n"
     ]
    }
   ],
   "source": [
    "# to fetch all the categories, names \n",
    "root = pathlib.Path(train_path)\n",
    "classes = sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
    "\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    ######################### Defining the constructor to extend the module class #####################\n",
    "    def __init__(self, num_classes =6):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # Output size after convolution filter\n",
    "        #((w - f + 2p) / s) + 1\n",
    "        \n",
    "        # In this we will specify all the layers in our networks\n",
    "        \n",
    "        #########################[Start] Adding first layer #####################\n",
    "        # Input shape = (batch size, #channels RGB, hight, width) i.e. (256,3,150,150)\n",
    "        self.convl1 = nn.Conv2d(in_channels= 3, out_channels= 12, kernel_size= 3, stride= 1, padding= 1)\n",
    "        # Now we have output channel is 12 then our so the shape changed to (256,12,150,150)\n",
    "        \n",
    "        # adding the batch Normalization\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=12)\n",
    "        # Note: after the batch normalization shape will be same. i.e. (256,12,150,150)\n",
    "        \n",
    "        # adding the ReLu to bring the non linearity\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # shape will be same (256,12,150,150)\n",
    "\n",
    "        # adding the maxpool, which will decrease the hight and width of Convolution by the factor of 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size= 2)\n",
    "        # shape (256, 12, 75, 75)\n",
    "        #########################[End] Adding first layer #####################\n",
    "        \n",
    "        #########################[Start] Adding second layer #####################\n",
    "        self.convl2 = nn.Conv2d(in_channels= 12, out_channels= 20, kernel_size= 3, stride= 1, padding= 1)\n",
    "        # shape (256,20,75,75)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # shape (256,20,75,75)\n",
    "        #########################[End] Adding second layer #####################\n",
    "        \n",
    "        #########################[Start] Adding second layer #####################\n",
    "        self.convl3 = nn.Conv2d(in_channels= 20, out_channels= 32, kernel_size= 3, stride= 1, padding= 1)\n",
    "        # shape (256,32,75,75)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=32)\n",
    "        # shape (256,32,75,75)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # shape (256,32,75,75)\n",
    "        #########################[End] Adding second layer #####################\n",
    "        \n",
    "        # Now we will add the fully connected layers\n",
    "        self.fc = nn.Linear(in_features= 32*75*75, out_features= num_classes)\n",
    "        \n",
    "    ######################### Defining the constructor to extend the module class #####################\n",
    "    \n",
    "    ################################## Defining the forward function ##################################\n",
    "    \n",
    "    def forward(self,input):\n",
    "        # passing all the inputs to the above layers\n",
    "        output = self.convl1()\n",
    "        output = self.bn1()\n",
    "        output = self.relu1()\n",
    "        output = self.pool()\n",
    "        \n",
    "        output = self.convl2()\n",
    "        output = self.relu2()\n",
    "        \n",
    "        output = self.convl3()\n",
    "        output = self.bn3()\n",
    "        output = self.relu3()\n",
    "        \n",
    "        #Note: above output will be in a shape of matrix form, shape (256,32,75,75)\n",
    "        \n",
    "        # This will change the view \n",
    "        output = output.view(-1, 32*75*75)\n",
    "        \n",
    "        # post that we will feed it inside the fully connected layers\n",
    "        output = output.fc(output)\n",
    "        \n",
    "        # return the final output\n",
    "        return output\n",
    "    \n",
    "    ################################## Defining the forward function ##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the ConvNet class\n",
    "model = ConvNet(num_classes= 6).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr= 0.001, weight_decay= 0.001)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining hyperparameter \n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14034 3000\n"
     ]
    }
   ],
   "source": [
    "# calculating the size of training and testing images\n",
    "train_count = len(glob.glob(train_path+'/**/*.jpg'))\n",
    "test_count = len(glob.glob(test_path+'/**/*.jpg'))\n",
    "\n",
    "print(train_count,test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b78135939bb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# This will give us a prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Calculating errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-45bf45b19956>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# passing all the inputs to the above layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvl1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'input'"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Evaluation and Training on training dataset\n",
    "    model.train()\n",
    "    train_accuracy = 0.0\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # initialization of batches inside the training\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda)\n",
    "            labels = Variable(labels.cuda)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # This will give us a prediction\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculating errors\n",
    "        loss = loss_function(outputs, labels)\n",
    "        \n",
    "        # this will do a back propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # this will update weight and bias using computed gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.cpu().data*images.size(0)\n",
    "        _, prediction = torch.max(outputs.data,1)\n",
    "        \n",
    "        train_accuracy += int(torch.sum(prediction == labels.data))\n",
    "        \n",
    "    train_accuracy = train_accuracy/train_count\n",
    "    train_loss = train_loss/train_count\n",
    "    \n",
    "    \n",
    "    # Evaluation on testing dataset\n",
    "    model.eval()\n",
    "    \n",
    "    test_accuracy = 0.0\n",
    "    \n",
    "    # initialization of batches inside the training\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda)\n",
    "            labels = Variable(labels.cuda)\n",
    "            \n",
    "        outputs += model(images)\n",
    "        _,prediction = torch.max(outputs.data,1)\n",
    "        \n",
    "        test_accuracy += int(torch.sum(prediction == labels.data))\n",
    "        \n",
    "    test_accuracy = test_accuracy/test_count\n",
    "    \n",
    "    print(\"Epoch: \"+str(epoch)+\" Train Loss: \"+str(int(train_loss))+\" Training Accuracy: \"+str(train_accuracy)+\" Test Accuracy: \"+str(test_accuracy))\n",
    "    \n",
    "    # saving the best model\n",
    "    if test_accuracy > best_accuracy:\n",
    "        torch.save(model.state_dict(),\"best_checkpoint.model\")\n",
    "        best_accuracy = test_accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
